{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed76ad2d",
   "metadata": {},
   "source": [
    "# Retail Saarthi SLM \n",
    "\n",
    "- This notebook contains us making a custom SLM for our final year Project \n",
    "\n",
    "## Step 1 : Load the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e897bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adhis\\OneDrive\\Desktop\\Retail-Saarthi-SLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local datasets...\n",
      "Loaded 1500 examples from SLM Training Dataset/Identity Dataset.csv\n",
      "Loaded 66 examples from SLM Training Dataset/Retail Term web dataset.csv\n",
      "Loaded 228 examples from SLM Training Dataset/Govt Act Data.csv\n",
      "Loaded 500 examples from SLM Training Dataset/Retail Comperhensive dataset.csv\n",
      "Loaded 93 examples from SLM Training Dataset/Audio Dataset.csv\n",
      "Total examples loaded: 2387\n",
      "Dataset ready for tokenization:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1909\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 478\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 1. Define your file list based on the uploaded files\n",
    "file_paths = [\n",
    "    \"SLM Training Dataset/Identity Dataset.csv\",\n",
    "    \"SLM Training Dataset/Retail Term web dataset.csv\",\n",
    "    \"SLM Training Dataset/Govt Act Data.csv\",\n",
    "    \"SLM Training Dataset/Retail Comperhensive dataset.csv\",\n",
    "    \"SLM Training Dataset/Audio Dataset.csv\"\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "# 2. Iterate through files and aggregate the 'text' column\n",
    "print(\"Loading local datasets...\")\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Ensure the 'text' column exists\n",
    "        if 'text' in df.columns:\n",
    "            # Drop any empty rows in the text column\n",
    "            cleaned_texts = df['text'].dropna().tolist()\n",
    "            all_texts.extend(cleaned_texts)\n",
    "            print(f\"Loaded {len(cleaned_texts)} examples from {file_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: No 'text' column found in {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "print(f\"Total examples loaded: {len(all_texts)}\")\n",
    "\n",
    "# 3. Create a Hugging Face Dataset\n",
    "full_dataset = Dataset.from_dict({\"text\": all_texts})\n",
    "\n",
    "# 4. Split into Train (80%) and Validation (20%) sets\n",
    "# We use a seed for reproducibility\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 5. Rename 'test' to 'validation' to match the notebook's expected structure\n",
    "ds = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': split_dataset['test']\n",
    "})\n",
    "\n",
    "print(\"Dataset ready for tokenization:\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a8ad5",
   "metadata": {},
   "source": [
    "## Step 2 : Tokenize the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532c6c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset (num_proc=4): 100%|██████████| 1909/1909 [00:12<00:00, 147.55 examples/s]\n",
      "Running tokenizer on dataset (num_proc=4): 100%|██████████| 478/478 [00:11<00:00, 41.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing train.bin: 100%|██████████| 1024/1024 [00:02<00:00, 496.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train.bin with 157916 tokens.\n",
      "Writing validation.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing validation.bin: 100%|██████████| 478/478 [00:00<00:00, 543.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation.bin with 40050 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# We will be using the 'gpt2' BPE tokenizer for this step as it is industry standard and mentioned in our reference [TinyStories Paper]\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Defining a preprocessing function to tokenize the text and convert it into token IDs\n",
    "def process(example,tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "):\n",
    "    ids = tokenizer.encode_ordinary(example[\"text\"])\n",
    "    out = {\"ids\":ids,\"len\":len(ids)}\n",
    "    return out\n",
    "\n",
    "#Apply the processing function to the entire dataset\n",
    "print(\"Tokenizing the dataset...\")\n",
    "tokenized=ds.map(\n",
    "    process,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "for split,dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'],dtype=np.uint64)\n",
    "    filename = f'{split}.bin'\n",
    "\n",
    "    dtype = np.uint16 ## As gpt2 bpe tokenizer has a vocab size of 50257, uint16 can easily accomodate it.\n",
    "\n",
    "    # Create a memory-mapped array on disk\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "\n",
    "    # To accomodate our small dataset [Temporary]\n",
    "    total_batches = min(1024, len(dset)) \n",
    "    if total_batches < 1:\n",
    "        total_batches = 1\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    print(f\"Writing {filename}...\")\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'Writing {filename}'):\n",
    "        # Batch together samples for faster write\n",
    "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        \n",
    "        # Write into mmap\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    \n",
    "    # Flush changes to disk\n",
    "    arr.flush()\n",
    "    print(f\"Saved {filename} with {arr_len} tokens.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312420c3",
   "metadata": {},
   "source": [
    "## STEP 3 - Creating input output Pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a2b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Batch Size: 32\n",
      "Block Size: 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "# Config \n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 128\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_type = 'cuda' if DEVICE == 'cuda' else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Block Size: {BLOCK_SIZE}\")\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    \n",
    "    ix = torch.randint(len(data)-BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([torch.from_numpy(data[i:i+BLOCK_SIZE]) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy(data[i+1:i+BLOCK_SIZE+1])for i in ix ])\n",
    "\n",
    "    if device_type == 'cuda':\n",
    "        x,y = x.pin_memory().to(DEVICE, non_blocking=True), y.pin_memory().to(DEVICE, non_blocking=True) \n",
    "    else:\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "    return x,y   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad01fe",
   "metadata": {},
   "source": [
    "# Step 4 : Define SLM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b4aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,ndim,bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim)) #STD = 1\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None # Mean = 0\n",
    "    def forward(self,x):\n",
    "        return F.layer_norm(x,self.weight.shape,self.weight,self.bias,1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd,3*config.n_embd,bias=config.bias) #Projection layer 768 -> 3*768 (for q,k,v)\n",
    "        self.c_proj = nn.Linear(config.n_embd,config.n_embd,bias=config.bias) # Output projection layer\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F,'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "        def forward(self,x):\n",
    "            B,T,C = x.size()\n",
    "            q,k,v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "            q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "            k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "            v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "\n",
    "            if self.flash:\n",
    "                y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "            else :\n",
    "                att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "                att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "                att = F.softmax(att, dim=-1)\n",
    "                att = self.attn_dropout(att)\n",
    "                y = att @ v\n",
    "\n",
    "            y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "            y = self.resid_dropout(self.c_proj(y))\n",
    "            return y\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))    \n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear(config.n_embd,4*config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, bias=False)\n",
    "        self.mlp =MLP(config)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            tok_emb = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            pos_emb = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.tok_emb.weight = self.lm_head.weight # Weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn,p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        \n",
    "    def _init_weights(self,module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.tok_emb(idx)\n",
    "        pos_emb = self.transformer.pos_emb(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f610714",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# DEFINE YOUR CONFIGURATION HERE\u001b[39;00m\n\u001b[32m      2\u001b[39m config = GPTConfig(\n\u001b[32m      3\u001b[39m     vocab_size=\u001b[32m50257\u001b[39m,     \u001b[38;5;66;03m# use the tokenizer's vocab size\u001b[39;00m\n\u001b[32m      4\u001b[39m     block_size=\u001b[32m128\u001b[39m,       \u001b[38;5;66;03m# or whatever context size you're training with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     bias=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m model = \u001b[43mGPT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mGPT.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.config = config\n\u001b[32m     93\u001b[39m \u001b[38;5;28mself\u001b[39m.transformer = nn.ModuleDict(\u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     94\u001b[39m     tok_emb = nn.Embedding(config.vocab_size, config.n_embd),\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     pos_emb = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_embd\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     96\u001b[39m     drop = nn.Dropout(config.dropout),\n\u001b[32m     97\u001b[39m     h = nn.ModuleList([Block(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config.n_layer)]),\n\u001b[32m     98\u001b[39m     ln_f = LayerNorm(config.n_embd, config.bias),\n\u001b[32m     99\u001b[39m ))\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.transformer.tok_emb.weight = \u001b[38;5;28mself\u001b[39m.lm_head.weight \u001b[38;5;66;03m# Weight tying\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adhis\\OneDrive\\Desktop\\Retail-Saarthi-SLM\\.venv\\Lib\\site-packages\\torch\\nn\\parameter.py:49\u001b[39m, in \u001b[36mParameter.__new__\u001b[39m\u001b[34m(cls, data, requires_grad)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.Tensor._make_subclass(\u001b[38;5;28mcls\u001b[39m, data, requires_grad)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Path for custom tensors: set a flag on the instance to indicate parameter-ness.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m t = \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m().requires_grad_(requires_grad)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(data):\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     52\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreating a Parameter from an instance of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     53\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequires that detach() returns an instance of the same type, but return \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mits __torch_dispatch__() implementation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     57\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'int' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "# DEFINE YOUR CONFIGURATION HERE\n",
    "config = GPTConfig(\n",
    "    vocab_size=50257,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Retail-Saarthi-SLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
