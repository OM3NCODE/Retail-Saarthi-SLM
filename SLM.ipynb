{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed76ad2d",
   "metadata": {},
   "source": [
    "# Retail Saarthi SLM \n",
    "\n",
    "- This notebook contains us making a custom SLM for our final year Project \n",
    "\n",
    "## Step 1 : Load the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e897bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adhis\\OneDrive\\Desktop\\Retail-Saarthi-SLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples with templates: 5387\n",
      "Dataset ready for tokenization:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4309\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1078\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 1. Define your file list based on the uploaded files\n",
    "file_paths = [\n",
    "    \"SLM Training Dataset/Identity Dataset.csv\",\n",
    "    \"SLM Training Dataset/Retail Term web dataset.csv\",\n",
    "    \"SLM Training Dataset/Govt Act Data.csv\",\n",
    "    \"SLM Training Dataset/Retail Comperhensive dataset.csv\",\n",
    "    \"SLM Training Dataset/Audio Dataset.csv\"\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "# Define a simple template\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'text' in df.columns:\n",
    "            # If it's the Identity or Govt Act file, use a Question-Answer format\n",
    "            if \"Identity\" in file_path or \"Govt\" in file_path:\n",
    "                for val in df['text'].dropna():\n",
    "                    formatted_text = f\"User: Who are you?\\nAssistant: {val}<|endoftext|>\\n\"\n",
    "                    all_texts.append(formatted_text)\n",
    "            else:\n",
    "                # For general retail terms, provide a generic prompt\n",
    "                for val in df['text'].dropna():\n",
    "                    formatted_text = f\"User: Explain this retail concept.\\nAssistant: {val}<|endoftext|>\\n\"\n",
    "                    all_texts.append(formatted_text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "# IMPORTANT: Duplicate the Identity data so the model prioritizes its Persona\n",
    "identity_df = pd.read_csv(\"SLM Training Dataset/Identity Dataset.csv\")\n",
    "for val in identity_df['text'].dropna():\n",
    "    # Adding it multiple times gives it more 'weight' in a small dataset\n",
    "    all_texts.append(f\"User: Who are you?\\nAssistant: {val}<|endoftext|>\\n\")\n",
    "    all_texts.append(f\"User: Who are you?\\nAssistant: {val}<|endoftext|>\\n\")\n",
    "\n",
    "print(f\"Total examples with templates: {len(all_texts)}\")\n",
    "\n",
    "# 3. Create a Hugging Face Dataset\n",
    "full_dataset = Dataset.from_dict({\"text\": all_texts})\n",
    "\n",
    "# 4. Split into Train (80%) and Validation (20%) sets\n",
    "# We use a seed for reproducibility\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 5. Rename 'test' to 'validation' to match the notebook's expected structure\n",
    "ds = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': split_dataset['test']\n",
    "})\n",
    "\n",
    "print(\"Dataset ready for tokenization:\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a8ad5",
   "metadata": {},
   "source": [
    "## Step 2 : Tokenize the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "532c6c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset (num_proc=4): 100%|██████████| 4309/4309 [00:08<00:00, 508.16 examples/s] \n",
      "Running tokenizer on dataset (num_proc=4): 100%|██████████| 1078/1078 [00:07<00:00, 152.55 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing train.bin: 100%|██████████| 1024/1024 [00:02<00:00, 453.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train.bin with 370030 tokens.\n",
      "Writing validation.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing validation.bin: 100%|██████████| 1024/1024 [00:01<00:00, 760.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation.bin with 93706 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# We will be using the 'gpt2' BPE tokenizer for this step as it is industry standard and mentioned in our reference [TinyStories Paper]\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# --- UPDATE IN STEP 2 ---\n",
    "def process(example,tokenizer):\n",
    "    # Use allowed_special to recognize the endoftext token\n",
    "    ids = tokenizer.encode_ordinary(example['text'])\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "    return out\n",
    "\n",
    "#Apply the processing function to the entire dataset\n",
    "print(\"Tokenizing the dataset...\")\n",
    "tokenized=ds.map(\n",
    "    process,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    "    num_proc=4,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    ")\n",
    "\n",
    "for split,dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'],dtype=np.uint64)\n",
    "    filename = f'{split}.bin'\n",
    "\n",
    "    dtype = np.uint16 ## As gpt2 bpe tokenizer has a vocab size of 50257, uint16 can easily accomodate it.\n",
    "\n",
    "    # Create a memory-mapped array on disk\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "\n",
    "    # To accomodate our small dataset [Temporary]\n",
    "    total_batches = min(1024, len(dset)) \n",
    "    if total_batches < 1:\n",
    "        total_batches = 1\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    print(f\"Writing {filename}...\")\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'Writing {filename}'):\n",
    "        # Batch together samples for faster write\n",
    "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        \n",
    "        # Write into mmap\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    \n",
    "    # Flush changes to disk\n",
    "    arr.flush()\n",
    "    print(f\"Saved {filename} with {arr_len} tokens.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312420c3",
   "metadata": {},
   "source": [
    "## STEP 3 - Creating input output Pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9a2b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Batch Size: 32\n",
      "Block Size: 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "# Config \n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 128\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_type = 'cuda' if DEVICE == 'cuda' else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Block Size: {BLOCK_SIZE}\")\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    \n",
    "    ix = torch.randint(len(data)-BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+BLOCK_SIZE]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+BLOCK_SIZE+1]).astype(np.int64)) for i in ix])\n",
    "\n",
    "    if device_type == 'cuda':\n",
    "        x,y = x.pin_memory().to(DEVICE, non_blocking=True), y.pin_memory().to(DEVICE, non_blocking=True) \n",
    "    else:\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "    return x,y   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad01fe",
   "metadata": {},
   "source": [
    "# Step 4 : Define SLM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5b4aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,ndim,bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim)) #STD = 1\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None # Mean = 0\n",
    "    def forward(self,x):\n",
    "        return F.layer_norm(x,self.weight.shape,self.weight,self.bias,1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd,3*config.n_embd,bias=config.bias) #Projection layer 768 -> 3*768 (for q,k,v)\n",
    "        self.c_proj = nn.Linear(config.n_embd,config.n_embd,bias=config.bias) # Output projection layer\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F,'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self,x):\n",
    "            B,T,C = x.size()\n",
    "            q,k,v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "            q = q.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "            k = k.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "            v = v.view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
    "\n",
    "            if self.flash:\n",
    "                y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "            else :\n",
    "                att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "                att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "                att = F.softmax(att, dim=-1)\n",
    "                att = self.attn_dropout(att)\n",
    "                y = att @ v\n",
    "\n",
    "            y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "            y = self.resid_dropout(self.c_proj(y))\n",
    "            return y\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self,x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))    \n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp =MLP(config)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            tok_emb = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            pos_emb = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.tok_emb.weight = self.lm_head.weight # Weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn,p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        \n",
    "    def _init_weights(self,module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.tok_emb(idx)\n",
    "        pos_emb = self.transformer.pos_emb(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f610714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE YOUR CONFIGURATION HERE\n",
    "config = GPTConfig(\n",
    "    vocab_size=50257,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=8,\n",
    "    n_head=8,\n",
    "    n_embd=128,\n",
    "    dropout=0.2,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cba6fc",
   "metadata": {},
   "source": [
    "## STEP 5 : Training config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc20bdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ee4e29b930>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from contextlib import nullcontext\n",
    "\n",
    "learning_rate = 5e-4\n",
    "max_iters = 2900\n",
    "warmup_steps = 100\n",
    "min_lr = 5e-5\n",
    "eval_iters = 200\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_type = 'cuda' if device == 'cuda' else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96b3607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(split)\n",
    "                with ctx:\n",
    "                    logits, loss = model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6ff91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "\n",
    "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n",
    "\n",
    "# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\n",
    "scaler = torch.amp.GradScaler(device_type, enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50878f16",
   "metadata": {},
   "source": [
    "## Pre Train SLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be8e9d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 97/2900 [00:04<02:05, 22.26it/s]c:\\Users\\adhis\\OneDrive\\Desktop\\Retail-Saarthi-SLM\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "  7%|▋         | 202/2900 [00:16<36:11,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: train loss 1.6657, val loss 1.7083\n",
      "The current learning rate: 0.00050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 403/2900 [00:33<33:18,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400: train loss 0.5789, val loss 0.5993\n",
      "The current learning rate: 0.00049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 601/2900 [00:50<31:25,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600: train loss 0.3721, val loss 0.4068\n",
      "The current learning rate: 0.00047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 802/2900 [01:07<29:01,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 800: train loss 0.3064, val loss 0.3416\n",
      "The current learning rate: 0.00043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1000/2900 [01:16<01:26, 21.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000: train loss 0.2738, val loss 0.3099\n",
      "The current learning rate: 0.00039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 1202/2900 [01:41<23:43,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1200: train loss 0.2514, val loss 0.2972\n",
      "The current learning rate: 0.00035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 1400/2900 [01:51<01:11, 20.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1400: train loss 0.2413, val loss 0.2864\n",
      "The current learning rate: 0.00030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 1600/2900 [02:08<01:00, 21.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1600: train loss 0.2329, val loss 0.2863\n",
      "The current learning rate: 0.00025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 1800/2900 [02:25<00:51, 21.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1800: train loss 0.2149, val loss 0.2807\n",
      "The current learning rate: 0.00020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 2001/2900 [02:49<11:55,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000: train loss 0.2113, val loss 0.2794\n",
      "The current learning rate: 0.00016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 2204/2900 [03:06<06:35,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2200: train loss 0.2017, val loss 0.2807\n",
      "The current learning rate: 0.00012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2402/2900 [03:23<06:34,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2400: train loss 0.2027, val loss 0.2733\n",
      "The current learning rate: 0.00008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 2602/2900 [03:40<04:25,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2600: train loss 0.1985, val loss 0.2678\n",
      "The current learning rate: 0.00006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2802/2900 [03:58<01:19,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2800: train loss 0.1851, val loss 0.2775\n",
      "The current learning rate: 0.00005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2900/2900 [04:02<00:00, 11.94it/s]\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list, validation_loss_list = [], []\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# In your training loop\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        # Ensure estimate_loss uses the correct device\n",
    "        losses = estimate_loss(model)\n",
    "        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "        train_loss_list += [losses['train']]\n",
    "        validation_loss_list += [losses['val']]\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    # Ensure X and y are on the correct device\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b4510",
   "metadata": {},
   "source": [
    "## Step 9: Plot the SLM Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86bd9bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.plot(validation_loss_list_converted, 'r', label='validation_loss')\n",
    "plt.xlabel(\"Steps - Every 100 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731d0ac",
   "metadata": {},
   "source": [
    "## RUN SLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44e8b305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adhis\\AppData\\Local\\Temp\\ipykernel_27692\\3676416321.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_params_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(config)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1fc582a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "��� aapko stock clear karne mein help karunga. Main lawyer nahi hoon, par guide kar sakta hoon.\n"
     ]
    }
   ],
   "source": [
    "# --- UPDATE IN STEP 6 ---\n",
    "model.eval()\n",
    "user_query = \"Baddd \"\n",
    "# Match the training template exactly\n",
    "#formatted_prompt = f\"User: {user_query}\\nAssistant: \"\n",
    "\n",
    "start_ids = tokenizer.encode(user_query, allowed_special={'<|endoftext|>'})\n",
    "context = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Use a lower temperature (0.2-0.3) for factual identity questions\n",
    "    y = model.generate(context, max_new_tokens=50, temperature=0.1)\n",
    "\n",
    "response = tokenizer.decode(y[0].tolist())\n",
    "# Extract only the assistant's new text\n",
    "print(response[len(user_query):].split(\"<|endoftext|>\")[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Retail-Saarthi-SLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
