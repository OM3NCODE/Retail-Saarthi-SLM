{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed76ad2d",
   "metadata": {},
   "source": [
    "# Retail Saarthi SLM \n",
    "\n",
    "- This notebook contains us making a custom SLM for our final year Project \n",
    "\n",
    "## Step 1 : Load the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e897bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adhis\\OneDrive\\Desktop\\Retail-Saarthi-SLM\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local datasets...\n",
      "Loaded 1500 examples from SLM Training Dataset/Identity Dataset.csv\n",
      "Loaded 66 examples from SLM Training Dataset/Retail Term web dataset.csv\n",
      "Loaded 228 examples from SLM Training Dataset/Govt Act Data.csv\n",
      "Loaded 500 examples from SLM Training Dataset/Retail Comperhensive dataset.csv\n",
      "Loaded 93 examples from SLM Training Dataset/Audio Dataset.csv\n",
      "Total examples loaded: 2387\n",
      "Dataset ready for tokenization:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1909\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 478\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# 1. Define your file list based on the uploaded files\n",
    "file_paths = [\n",
    "    \"SLM Training Dataset/Identity Dataset.csv\",\n",
    "    \"SLM Training Dataset/Retail Term web dataset.csv\",\n",
    "    \"SLM Training Dataset/Govt Act Data.csv\",\n",
    "    \"SLM Training Dataset/Retail Comperhensive dataset.csv\",\n",
    "    \"SLM Training Dataset/Audio Dataset.csv\"\n",
    "]\n",
    "\n",
    "all_texts = []\n",
    "\n",
    "# 2. Iterate through files and aggregate the 'text' column\n",
    "print(\"Loading local datasets...\")\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Ensure the 'text' column exists\n",
    "        if 'text' in df.columns:\n",
    "            # Drop any empty rows in the text column\n",
    "            cleaned_texts = df['text'].dropna().tolist()\n",
    "            all_texts.extend(cleaned_texts)\n",
    "            print(f\"Loaded {len(cleaned_texts)} examples from {file_path}\")\n",
    "        else:\n",
    "            print(f\"Warning: No 'text' column found in {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "print(f\"Total examples loaded: {len(all_texts)}\")\n",
    "\n",
    "# 3. Create a Hugging Face Dataset\n",
    "full_dataset = Dataset.from_dict({\"text\": all_texts})\n",
    "\n",
    "# 4. Split into Train (80%) and Validation (20%) sets\n",
    "# We use a seed for reproducibility\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 5. Rename 'test' to 'validation' to match the notebook's expected structure\n",
    "ds = DatasetDict({\n",
    "    'train': split_dataset['train'],\n",
    "    'validation': split_dataset['test']\n",
    "})\n",
    "\n",
    "print(\"Dataset ready for tokenization:\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a8ad5",
   "metadata": {},
   "source": [
    "## Step 2 : Tokenize the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532c6c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing the dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset (num_proc=4): 100%|██████████| 1909/1909 [00:12<00:00, 147.55 examples/s]\n",
      "Running tokenizer on dataset (num_proc=4): 100%|██████████| 478/478 [00:11<00:00, 41.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing train.bin: 100%|██████████| 1024/1024 [00:02<00:00, 496.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train.bin with 157916 tokens.\n",
      "Writing validation.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing validation.bin: 100%|██████████| 478/478 [00:00<00:00, 543.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation.bin with 40050 tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# We will be using the 'gpt2' BPE tokenizer for this step as it is industry standard and mentioned in our reference [TinyStories Paper]\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Defining a preprocessing function to tokenize the text and convert it into token IDs\n",
    "def process(example,tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "):\n",
    "    ids = tokenizer.encode_ordinary(example[\"text\"])\n",
    "    out = {\"ids\":ids,\"len\":len(ids)}\n",
    "    return out\n",
    "\n",
    "#Apply the processing function to the entire dataset\n",
    "print(\"Tokenizing the dataset...\")\n",
    "tokenized=ds.map(\n",
    "    process,\n",
    "    remove_columns=['text'],\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "for split,dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'],dtype=np.uint64)\n",
    "    filename = f'{split}.bin'\n",
    "\n",
    "    dtype = np.uint16 ## As gpt2 bpe tokenizer has a vocab size of 50257, uint16 can easily accomodate it.\n",
    "\n",
    "    # Create a memory-mapped array on disk\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "\n",
    "    # To accomodate our small dataset [Temporary]\n",
    "    total_batches = min(1024, len(dset)) \n",
    "    if total_batches < 1:\n",
    "        total_batches = 1\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    print(f\"Writing {filename}...\")\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'Writing {filename}'):\n",
    "        # Batch together samples for faster write\n",
    "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        \n",
    "        # Write into mmap\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    \n",
    "    # Flush changes to disk\n",
    "    arr.flush()\n",
    "    print(f\"Saved {filename} with {arr_len} tokens.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312420c3",
   "metadata": {},
   "source": [
    "## STEP 3 - Creating input output Pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a2b572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Batch Size: 32\n",
      "Block Size: 128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "# Config \n",
    "BATCH_SIZE = 32\n",
    "BLOCK_SIZE = 128\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device_type = 'cuda' if DEVICE == 'cuda' else 'cpu'\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Block Size: {BLOCK_SIZE}\")\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    \n",
    "    ix = torch.randint(len(data)-BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([torch.from_numpy(data[i:i+BLOCK_SIZE]) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy(data[i+1:i+BLOCK_SIZE+1])for i in ix ])\n",
    "\n",
    "    if device_type == 'cuda':\n",
    "        x,y = x.pin_memory().to(DEVICE, non_blocking=True), y.pin_memory().to(DEVICE, non_blocking=True) \n",
    "    else:\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "    return x,y   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Retail-Saarthi-SLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
